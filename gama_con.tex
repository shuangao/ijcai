In this paper, we present a novel method called SMITLe that is able to transfer knowledge across different datasets and learn a new category. Inspired by previous work, SMITLe uses LS-SVM as the basic classification model and LOO for transfer parameter estimation. We demonstrate that SMITLe is able to converge at a logarithmic rate. We also prove that with the transfer parameters optimized by our novel objective function, SMITLe is able to avoid negative transfer which is a general issue for transfer learning. 
We carry out 3 sets of experiment that our algorithm would face in real world application. 
From the experimental results we can see SMITLe can consistently outperform other transfer baselines and achieve higher classification accuracy in different scenarios.